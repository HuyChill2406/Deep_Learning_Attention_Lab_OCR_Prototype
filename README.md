# Deep_Learning_Attention_Lab_OCR_Prototype
Attention variants (Self/MQA/GQA/Sparse/Linear) study + OCR CNN-to-Transformer prototype.

ğŸ’¡ Tá»•ng quan

Notebook gá»“m 2 pháº§n:

Attention Lab: so sÃ¡nh Self-Attention, MQA, GQA, Sparse/Strided, Linear/Performer (Ä‘á»™ phá»©c táº¡p, bá»™ nhá»›, tá»‘c Ä‘á»™).

OCR Prototype: kiáº¿n trÃºc CNN/ResNet Encoder + Transformer Decoder cho bÃ i toÃ¡n Imageâ†’Text (nháº­n dáº¡ng kÃ½ tá»±).

ğŸ” Insight chÃ­nh

MQA/GQA: chia sáº» K/V giá»¯a heads â‡’ giáº£m chi phÃ­ suy luáº­n khi sá»‘ head lá»›n.

Sparse/Strided: phÃ¹ há»£p chuá»—i ráº¥t dÃ i.

Linear/Performer: gáº§n tuyáº¿n tÃ­nh theo chiá»u dÃ i chuá»—i â‡’ há»£p tÃ i nguyÃªn háº¡n cháº¿.

ğŸ§± OCR: Kiáº¿n trÃºc & huáº¥n luyá»‡n

Encoder: ResNet-style CNN trÃ­ch xuáº¥t feature map.

Decoder: Transformer Decoder sinh chuá»—i kÃ½ tá»±; so sÃ¡nh nhanh hÆ°á»›ng CRNN/CTC.

Huáº¥n luyá»‡n: Adam/SGD, early-stopping, checkpoint â€œbestâ€.

ÄÃ¡nh giÃ¡: loss, accuracy theo kÃ½ tá»± vÃ  chuá»—i; phÃ¢n tÃ­ch lá»—i kÃ½ tá»± dá»… nháº§m.

ğŸ“Š Káº¿t quáº£

Attention: MQA/GQA nhanh hÆ¡n Multi-Head chuáº©n khi seq_len dÃ i & n_heads lá»›n.

OCR: Ä‘á»™ chÃ­nh xÃ¡c chuá»—i (CER/WER) cáº£i thiá»‡n khi tÄƒng augment + beam search.

ğŸ“ Ghi chÃº ká»¹ thuáº­t

Äáº·t seed Ä‘á»ƒ tÃ¡i láº­p.

Log thá»i gian/bá»™ nhá»› khi so sÃ¡nh attention.

Vá»›i OCR: thÃªm augmentation (affine/gaussian), beam search, vocab chuyÃªn biá»‡t.
